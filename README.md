## Deep Learning Compiler Study
This is a repository of the study "DL Compiler". The goal of this study is to understand the acceleration of nerual networks with DL Compiler. The topic of acceleration includes `On-Device AI`,`DL Compiler`, `TVM`, `ONNX` , `Compiler`. Our materials are open to git and youtube. 


Our study is based on this paper (`The Deep Learning Compiler: A Comprehensive Survey`, IEEE TPDS 2021).
## Paper List
|Paper Name|Conference/Jounr Name|Year|
|---|---|---|
|TVM: An automated end-to-end optimizing compiler for deep learning|OSDI|2018|
|XLA| https://www.tensorflow.org/xla?hl=ko| 2017|
|Efficient Execution of Quantized Deep Learning Models: A Compiler Approach| https://arxiv.org/abs/2006.10226|2020|
|PPlaidML| https://www.intel.com/content/www/us/en/artificial-intelligence/plaidml.html| 2017|
|Learning to Optimize Tensor Programs & Ansor| NIPS & OSDI |2020|
|Glow: Graph Lowering Compiler Techniques for Neural Networks| https://arxiv.org/pdf/1805.00907.pdf| 2018|


|MLIR: Scaling Compiler Infrastructure for Domain Specific Computation|CGO|2021|IR|

## Presentation with Video
# TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
	Presenter: Constant Park (sonicstage12@naver.com)
	Date: February, 25, 2021
	PPT: https://github.com/ConstantPark/DL_Compiler/blob/main/TVM.pdf
	Video: https://youtu.be/wzy1QMci_Zs

# XLA: Optimizing Compiler for Machine Learning
	Presenter: Tee Jung (naey05@gmail.com, https://b.mytears.org/)
	Date: March, 11, 2021
	PPT: https://github.com/ConstantPark/DL_Compiler/blob/main/XLA101.pdf
	Video: https://youtu.be/_3ykXQH5h2o

# Efficient Execution of Quantized Deep Learning Models: A Compiler Approach
	Presenter: 이제민 (leejaymin@cnu.ac.kr)
	Date: March, 25, 2021
	PPT: 
	Video: 
	
## Contributors
**Main Contributor**: Constant Park (sonicstage12@naver.com)
**Presenters**: Constanr Park (sonicstage12@naver.com), 이제민 (leejaymin@cnu.ac.kr), 정태영 (naey05@gmail.com)

