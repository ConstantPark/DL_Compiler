## Deep Learning Compiler Study
This is a repository of the study "DL Compiler". The goal of this study is to understand the acceleration of nerual networks with DL Compiler. The topic of acceleration includes `On-Device AI`,`DL Compiler`, `TVM`, `ONNX` , `Compiler`. Our materials are open to git and youtube. 


Our study is based on this paper (`The Deep Learning Compiler: A Comprehensive Survey`, IEEE TPDS 2021).
## Paper List
|Paper Name|Conference/Jounr Name|Year|Keyword|
|---|---|---|---|
|Learning to Optimize Tensor Programs|NIPS|2018|Scheduling|
|TVM: An automated end-to-end optimizing compiler for deep learning|OSDI|2018|DL Compiler|
|MLIR: Scaling Compiler Infrastructure for Domain Specific Computation|CGO|2021|IR|

## Presentation with Video
# TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
	Presenter: Constant Park (sonicstage12@naver.com)
	Date: February, 25, 2021
	PPT: https://github.com/ConstantPark/DL_Compiler/blob/main/TVM.pdf
	Video: https://youtu.be/wzy1QMci_Zs

# XLA: Optimizing Compiler for Machine Learning
	Presenter: Tee Jung (naey05@gmail.com, https://b.mytears.org/)
	Date: March, 11, 2021
	PPT: https://github.com/ConstantPark/DL_Compiler/blob/main/XLA101.pdf
	Video: 

## Contributors
**Main Contributor**: Constant Park (sonicstage12@naver.com)
**Presenters**: Constanr Park (sonicstage12@naver.com), 이제민 (leejaymin@cnu.ac.kr), 정태영 (naey05@gmail.com)

